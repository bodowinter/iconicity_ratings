---
title: "Iconicity ratings - processing & exclusions"
author: "Bodo Winter"
date: "10/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This script performs the preprocessing of the raw ratings. It takes as input the final `raw_ratings_full_01_12_2022.csv` dataset, which includes every single trial from the rating study. This dataset, however, also contains MTurk IDs that could be used to identify people. We therefore first anonymize the data. The parts that contain the anonymization are left in the script for completeness, but executed with `eval = FALSE` in the final version as we do not share the de-anonymized data.

The remainder of the manuscript performs a series of data exclusions, as described in the paper. The total number of resulting datasets are three-fold:

1) a raw trial data file that includes everything that has been collected, anonymized, and without exclusions applied
2) a raw trial data file, anonymized, that includes only those trials that are included in our calculations of the iconicity rating averages
3) a summary file with by-word averages and other summary statistics based on the trial data after exclusion criteria have been applied

## Setup

Load packages:

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
```

Print versions to report for reproducibility:

```{r}
packageVersion('tidyverse')
R.Version()$version.string
```

This won't be repeated in the other scripts. It's done in this script so that we can report R and tidyverse package in the main manuscript.

## Anonymization

Load data. We need to hand-specify the column parsing because otherwise a lot of subjects will be put to NA because the first few rows are integer and `read_csv()` will interpret those rows as integers and set all characters to NA.

This is the non-anonymized data we won't load in the final version:

```{r, eval = FALSE}
df <- read_csv('../ratings/raw_ratings_full_01_12_2022.csv',
               col_types = cols(row_id = col_integer(),
                                subj_code = col_character(),
                                batch_num = col_integer(),
                                bin = col_integer(),
                                word = col_character(),
                                key = col_double(),
                                rt = col_double()))
```

This is what will be loaded in the final version:

```{r, warning = FALSE, message = FALSE}
df <- read_csv('../ratings/raw_ratings_full_anonymized_01_12_2022.csv',
               col_types = cols(row_id = col_integer(),
                                subj_code = col_character(),
                                word = col_character(),
                                key = col_double(),
                                rt = col_double()))
```

Create anonymized identifiers for each subject. This will be included in the script but it is essentially ineffective in this case (since only the anonymized data will be shared in the repo). So this code chunk is set to `eval = FALSE` in the knitted document that will be published.

```{r, eval = FALSE}
# Table with new ids:

new_ids <- tibble(subj_code = unique(df$subj_code), # original codes
                  sub_ID = 1:length(unique(df$subj_code))) # new codes

# Override old IDs:

df <- df %>%
  left_join(new_ids) %>%
  select(-subj_code) %>% 
  rename(subj_code = sub_ID) %>% 
  select(row_id, subj_code, word:rt)

# Override file:

write_csv(df,
          '../ratings/raw_ratings_full_anonymized_01_12_2022.csv')
```

## Check total data for reporting

Check total number of data points:

```{r}
nrow(df)
```

181,932 now: after review, including the final batch.

Are there any NAs for the `subj_code` column?

```{r}
filter(df, is.na(subj_code))
```

None.

Check total number of participants:

```{r}
df %>%
  count(subj_code) %>%
  nrow()
```

1419 people in total.

Before we do the substantive data exclusions (those that are theoretically motivated), we'll get rid of four data points that have `NA` for the `rt` and `key` columns. For these data points, the most likely explanation is that the browser crashed. Since we have no data for these trials, there's no point in included them in our count for the total. That is, we don't want our total to include the four missing values.

How many are `NA`?

```{r}
filter(df, is.na(rt))
```

**Exclusion 0**: Exclude the missing values:

```{r}
df <- filter(df,
             !is.na(rt))
```

Everything from now on will be a substantive exclusion of data, that is, something that actually impacts our conclusions, and the numbers we report in the data.

## Exclusion 1: Reaction times

Establish baseline for RT exclusions. This will be used for computing N of trials lost:

```{r}
baseline <- nrow(df)

# Show:

baseline
```

181928 data points.

Visualize the overall RT distribution:

```{r, fig.width = 8, fig.height = 6}
df %>%
  ggplot(aes(x = log(rt))) +
  geom_density(fill = 'steelblue', alpha = 0.5) +
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 0.9)) +
  theme_classic()
```

Check the reaction time mean and SD:

```{r}
df %>%
  summarize(M = mean(rt),
            SD = sd(rt))
```

That's a massive standard deviation. What's the range?

```{r}
range(df$rt)
```

Crazy spread. The minimum number (6ms) is too low for sure. Nobody can process a word and click that quickly.

Quickly check those responses that are below 100 or below 500:

```{r}
# Below 100ms:

filter(df, rt < 100) %>%
  count(subj_code, sort = TRUE)

# Below 500ms:

filter(df, rt < 500) %>%
  count(subj_code, sort = TRUE)
```

There definitely seems to be a few free riders here, that is, people just clicking themselves through the experiment.

How many data points have response times below 100ms? Below 200ms? 300ms? 500ms?

```{r}
filter(df, rt < 100) %>% nrow()
filter(df, rt < 200) %>% nrow()
filter(df, rt < 300) %>% nrow()
filter(df, rt < 500) %>% nrow()
filter(df, rt < 500) %>% nrow() / nrow(df) # proportion
```

Even 500ms is just about 2% of the data. Selecting any one particular value for thresholding is effectively arbitrary, but 500ms seems like a nice round number in the range that we definitely want to include. It is next to nigh impossible to make a semantically deep judgment involving a comparison of phonological form to semantics in such a short time span. So, let's do **Exclusion 2** based on RTs:

```{r}
# Exclusion 2... based on rt < 500ms

df <- filter(df, rt > 500)
```

**Note:** We previously excluded words based on a higher threshold as well, but the reviewers flagged this as unmotivated.

How much exclusion purely based on RT?

```{r}
baseline - nrow(df)
(baseline - nrow(df)) / nrow(df)
```

2.2% data loss, a total of 3901 data points.

Set N after RT exclusion as baseline for reporting:

```{r}
new_baseline <- nrow(df)
```

## Exclusion 2: Straightlining

Let's check whether there are any people who have had the same response for every value.

First, we'll get a count of N per subject:

```{r}
N_sub <- df %>%
  count(subj_code) %>% 
  rename(total = n)
```

Then the count of subject/key combinations:

```{r}
N_resp <- df %>%
  count(subj_code, key, sort = TRUE)

# Check:

N_resp
```

Subject `468` for example rated 137 words with an iconicity rating of `7`. That's a bit suspicious. Let's merge this with the per-subject-N to see the proportion of responses that are the same.

```{r}
N_resp <- N_resp %>%
  left_join(N_sub) %>% 
  mutate(prop = n / total)
```

Look at the distribution of this:

```{r, fig.width = 8, fig.height = 6}
N_resp %>%
  ggplot(aes(x = prop)) +
  geom_density(fill = 'steelblue', alpha = 0.5) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 5)) +
  theme_classic()
```

Compute average reaction time per subject:

```{r}
df_sub <- df %>% 
  group_by(subj_code) %>% 
  summarize(rt = mean(rt))
```

Check whether there is a correlation between response speed and proportion of same-value-responses:

```{r}
N_resp <- left_join(N_resp, df_sub,
                    by = c('subj_code' = 'subj_code'))

# Correlate:

with(N_resp, cor(prop, rt))
```

Nothing much in the correlation coefficient. But let's plot this just in case:

```{r, fig.width = 8, fig.height = 6}
N_resp %>%
  ggplot(aes(x = prop, y = rt)) +
  geom_point(alpha = 0.6) +
  theme_classic()
```

This is an interesting distribution that clearly shows that the subjects with a high proportion of "same" responses are a bit problematic: There is a lot of variation in response speed for people with very mixed proportions (to the left). Not a lot of variation for people with very high proportion though (a striking absence of slow responders here). This suggests that the people with high proportions of the same response may include a lot of people clicking themselves through the experiment.

Based on this image, excluding people based on a 80% cut-off value would not entail a lot of data loss. So this seems rather innocuous, but given the reaction times, well-motivated.

```{r}
bad_subs <- filter(N_resp, prop > 0.8) %>%
  pull(subj_code)

# How many?

length(bad_subs)

# Out of?

length(unique(df$subj_code))
length(bad_subs) / length(unique(df$subj_code))
```

16 out of 1419 participants will be excluded, that's 1%.

Exclude them:

```{r}
# Exclusion 4... based on prop > 0.80

df <- filter(df, !(subj_code %in% bad_subs))

# How much is that from the baseline?

new_baseline - nrow(df)
1 - (nrow(df) / (new_baseline))
```

1.1% exclusion.

Reset baseline for next comparison:

```{r}
new_baseline <- nrow(df)
```

## Exclusion 3: Correlation with the mean

Following Warriner et al. (2013), we exclude participants who correlated with the item-based average less than 0.1.

First compute averages:

```{r}
df_avg <- df %>%
  group_by(word) %>% 
  summarize(key_M = mean(key, na.rm = TRUE))
```

Append the means into the main data frame for ease of processing:

```{r}
df <- left_join(df, df_avg)
```

For each participant, get the correlation between them and those averages. First, setup a data frame where each row is one participant:

```{r}
ppt_corrs <- tibble(subj_code = unique(df$subj_code))

# Append columns of NAs to be filled with correlation coefficients:

ppt_corrs$r = numeric(nrow(ppt_corrs))
```

Then, loop through that and compute the correlations:

```{r}
for (i in 1:nrow(ppt_corrs)) {
  # Extract subject into subset:
  
  id <- ppt_corrs[i, ]$subj_code
  this_df <- filter(df, subj_code == id)
  
  # Compute and store correlation:
  
  ppt_corrs[i, ]$r <- with(this_df,
                           cor(key, key_M,
                               use = 'complete.obs'))
}
```

Check the distribution:

```{r, fig.width = 8, fig.height = 6}
ppt_corrs %>%
  ggplot(aes(x = r)) +
  geom_density(fill = 'steelblue', alpha = 0.5) +
  geom_vline(aes(xintercept = 0), linetype = 2) +
  geom_vline(aes(xintercept = 0.1), col = 'darkgrey') + # threshold value r = 0.1
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 3)) +
  theme_classic()
```

Good! Most people were actually quite well correlated with the average, suggesting that they did something meaningful, and that there's a bit of wisdom-of-the-crowd going on here. But some people clearly did something weird that was anti-correlated with the rest. It would make sense to include that, also following other studies that have done the same. We're not going to go as far as flipping the values for some of the negatively correlated subjects, as we cannot *know* that they just interpreted the scale the other way round.

What's the average of the correlations?

```{r}
ppt_corrs %>%
  summarize(r_M = mean(r),
            r_SD = sd(r))
```

Sort this in ascending order:

```{r}
ppt_corrs <- arrange(ppt_corrs, r)

# Show:

ppt_corrs
```

Get the bad subs, using a threshold of r = 0.1:

```{r}
bad_subs <- filter(ppt_corrs, r < 0.1) %>%
  pull(subj_code)
```

How many are these of the total?

```{r}
length(bad_subs)
length(unique(df$subj_code))
length(bad_subs) / length(unique(df$subj_code))
```

That's 70 participants, 4.9% of the total number of participants.

Get rid of those **Exclusion 3**:

```{r}

df <- filter(df,
             !(subj_code %in% bad_subs))
```

How much exclusion?

```{r}
new_baseline - nrow(df)
1 - (nrow(df) / new_baseline)
```

7,8585 data points, 4.4% of the data.

## Exclusion 4: Exclude based on lacking word knowledge

Check how many `NA` values there are per word:

```{r}
NA_count <- df %>%
  mutate(key_NA = ifelse(is.na(key), 1, 0)) %>% 
  group_by(word) %>% 
  summarize(NA_count = sum(key_NA))
```

Append overall count:

```{r}
# Compute counts of ratings per word:

word_count <- df %>% count(word)

# Append this overall count to NA count tibble:

NA_count <- left_join(NA_count, word_count)
```

Compute proportion of N known:

```{r}
NA_count <- NA_count %>%
  mutate(prop_known = NA_count / n,
         prop_known = 1 - prop_known)
```

Look at the distribution of this via summary statistics:

```{r}
NA_count %>%
  summarize(M = mean(prop_known),
            min = min(prop_known),
            max = max(prop_known))
```

The average is close to 1 indicating that most words are known. Let's have a closer look at the distribution:

```{r, fig.width = 8, fig.height = 6}
NA_count %>%
  ggplot(aes(x = prop_known)) +
  geom_density(fill = 'steelblue', alpha = 0.5) +
  scale_y_continuous(limits = c(0, 40),
                     expand = c(0, 0)) +
  theme_classic()
```

How many are known by 80% of all participants? And how many are not?

```{r}
# How many are above 80%?

filter(NA_count, prop_known > 0.80) %>% nrow()

# How many are below 80%?

filter(NA_count, prop_known <= 0.80) %>% nrow()

# Percentage out of total:

1 - NA_count %>% filter(prop_known > 0.80) %>% nrow() / nrow(NA_count)
```

618 words, that would be a loss of 4.0%

Extract the words that are known by less than 80% of all participants:

```{r}
bad_words <- NA_count %>%
  filter(prop_known <= 0.80) %>%
  pull(word)
```

Show sample:

```{r}
bad_words
```

How many?

```{r}
length(bad_words)
```

Exclude those, which is **Exclusion 4**:

```{r}
df <- filter(df,
             !(word %in% bad_words))
```

How much exclusion?

```{r}
new_baseline - nrow(df)
1 - (nrow(df) / new_baseline)
```

14,937 data points, which is 8.5% percentage. This is the so far biggest exclusion, but those words are kind of crap anyway. We can't trust them if people don't know them, and it also makes it quite likely that for those people who were perhaps not as hesitant, they may have provided ratings that were not particularly meaningful.

## Compute item-based averages and SDs:

Re-compute averages:

```{r}
icon <- df %>%
  group_by(word) %>% 
  summarize(rating = mean(key, na.rm = TRUE),
            rating_sd = sd(key, na.rm = TRUE))
```

Append known / not known data:

```{r}
icon <- left_join(icon, NA_count)
```

Rename and re-order:

```{r}
icon <- icon %>% 
  mutate(n_ratings = n - NA_count) %>% 
  select(word, n_ratings, n, prop_known, rating, rating_sd)
```

What is the average number of ratings per word?

```{r}
icon %>%
  summarize(n = mean(n_ratings),
            min = min(n_ratings),
            max = max(n_ratings))
```

What's with those low ones? They were all known, but why do we have so few data points for these?

```{r}
arrange(icon, n_ratings)
```

How many will be excluded if we use the >= 10 ratings criterion?

```{r}
filter(icon, n < 10) %>% nrow()
```

95 words.

For reporting, these words need to be considered in the final amount of data exclusion, so we need to look them up in the raw rating dataset (before averaging) to look at the precise number of trials excluded:

```{r}
final_words <- icon$word
final_N_trials <- filter(df, word %in% final_words) %>% nrow()

# Final ratings:

final_N_trials

# Final exclusion and percentage exclusion:

baseline - final_N_trials
1 - (final_N_trials / baseline)
```

11.0% in total, 20,871 raw trials.

Save the average data:

```{r}
write_csv(icon, '../ratings/iconicity_ratings_cleaned.csv')
```

Also get the raw ratings for which we have enough (>10) and then also get rid of NAs for that.

```{r}
write_csv(filter(df,
                 # !is.na(key),
                 word %in% final_words),
          '../ratings/iconicity_ratings_raw.csv')
```

This completes this script.
