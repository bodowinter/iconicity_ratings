---
title: "Iconicity ratings - substantive analysis"
author: "Bodo"
date: "11/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This markdown performs a number of different analysis that serve to demonstrate how the iconicity ratings correlate with other psycholinguistic norms that have been collected. In addition, we produce some plots of the data to give an overview of the ratings.

The models are computed in the file "Bayesian_models.Rmd" and will be loaded in and interpreted here.

## Data and package loading

Load packages:

```{r, warning = FALSE, message = FALSE}
library(brms, quietly = TRUE)
library(tidyverse, quietly = TRUE)
library(patchwork, quietly = TRUE)
```

For reproducibility:

```{r}
packageVersion('brms')
packageVersion('tidyverse')
packageVersion('patchwork')
R.Version()$version.string
```

Load iconicity ratings:

```{r, warning = FALSE, message = FALSE}
icon <- read.csv('../ratings/iconicity_ratings.csv')
```

Load additional datasets. Getting "buffer" errors with `read.csv()`, so will use `read.csv()` and `as_tibble()` for now.

```{r, warning = FALSE, message = FALSE}
# Data for replications:

SER <- read.csv('../additional_data/juhasz_yap_2013_SER.csv',
                stringsAsFactors = FALSE) %>% as_tibble()
AOA <- read.csv('../additional_data/kuperman_2012_AOA.csv',
                stringsAsFactors = FALSE) %>% as_tibble()
lanc <- read.csv('../additional_data/lancaster_sensorimotor_norms_2019.csv',
                stringsAsFactors = FALSE) %>% as_tibble()
SUBTL <- read.csv('../additional_data/brysbaert_2012_SUBTLEX_POS.csv',
                stringsAsFactors = FALSE) %>% as_tibble()
humor <- read.csv('../additional_data/engelthaler_hills_2018_humor.csv',
                stringsAsFactors = FALSE) %>% as_tibble()
ding <- read.csv('../additional_data/dingemanse_thompson_2020.csv',
                stringsAsFactors = FALSE) %>% as_tibble()

# Data for controlling morphology:

ELP <- read.csv('../additional_data/balota_2007_ELP.csv',
                stringsAsFactors = FALSE) %>% as_tibble()

# Data for new analyses:

levin <- read.csv('../additional_data/levin_1991_verb_classes.csv',
                stringsAsFactors = FALSE) %>% as_tibble()
```

Rename folders and simplify data frames to include only relevant info. Also make Lancaster norm Word column lowercase. Log10 transform SUBTLEX frequencies and contextual diversity:

```{r}
# Age-of-acquisition data:

AOA <- AOA %>% select(Word, Rating.Mean) %>% 
  rename(AOA = Rating.Mean)

# Sensory experience ratings:

SER <- select(SER, Word, SER)

# Lancaster sensorimotor norms:

lanc <- lanc %>%
  mutate(Word = str_to_lower(Word)) %>% 
  select(Word:Visual.mean, Dominant.perceptual,
         Max_strength.perceptual) %>% 
  rename(Aud = Auditory.mean,
         Gus = Gustatory.mean,
         Hap = Haptic.mean,
         Int = Interoceptive.mean,
         Olf = Olfactory.mean,
         Vis = Visual.mean,
         Mod = Dominant.perceptual,
         Max_perceptual = Max_strength.perceptual)

# Frequency, contextual diversity, and part-of-speech:

SUBTL <- SUBTL %>% 
  rename(Freq = FREQcount,
         CD = CDcount,
         POS = Dom_PoS_SUBTLEX) %>% 
  select(Word, Freq, CD, POS)

# Playfulness:
  
humor <- select(humor, word, mean) %>%
  rename(humor = mean)

# Dingemanse & Thompson (2020) data:

ding <- select(ding, logletterfreq, word, ico, ico_imputed, ico_imputed_monomorph)

# ELP data:

ELP <- select(ELP, Word, NMorph) %>% 
  mutate(LogMorph = log10(NMorph),
         Word = str_to_lower(Word))
```

Join them into the main iconicity data file:

```{r}
icon <- left_join(icon, SER, by = c('word' = 'Word'))
icon <- left_join(icon, AOA, by = c('word' = 'Word'))
icon <- left_join(icon, SUBTL, by = c('word' = 'Word'))
icon <- left_join(icon, humor, by = c('word' = 'word'))
icon <- left_join(icon, lanc, by = c('word' = 'Word'))
icon <- left_join(icon, ding, by = c('word' = 'word'))
icon <- left_join(icon, ELP, by = c('word' = 'Word'))
```

For SUBTLEX, an NA is a true zero:

```{r}
icon <- mutate(icon,
               Freq = ifelse(is.na(Freq), 0, Freq),
               CD = ifelse(is.na(CD), 0, CD))
```

Log-transform the frequencies:

```{r}
icon <- mutate(icon,
               LogFreq = log10(Freq + 1),
               LogCD = log10(CD + 1))
```

Z-score all variables:

```{r}
z_score <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x)

icon <- mutate(icon,
               SER_z = z_score(SER),
               AOA_z = z_score(AOA),
               LogFreq_z = z_score(LogFreq),
               LogCD_z = z_score(LogCD),
               humor_z = z_score(humor),
               logletter_z = z_score(logletterfreq),
               lognmorph_z = z_score(LogMorph),
               
               # Lancaster norms:
               Aud_z = z_score(Aud),
               Gus_z = z_score(Gus),
               Hap_z = z_score(Hap),
               Int_z = z_score(Int),
               Olf_z = z_score(Olf),
               Vis_z = z_score(Vis))
```

## Process part of speech tags

Process the part-of-speech information to collapse categories for better representation. First show what categories there are:

```{r}
sort(table(icon$POS))
```

Define vector of stuff to set as function words. "Ex" = there. "#N/A" are words like "gonna", "wanna". 

```{r}
gram <- c('#N/A', 'Article', 'Conjunction',
          'Determiner', 'Not', 'Number',
          'Preposition', 'Pronoun', 'To',
          'Ex')
```

Set this to function words in a new POS variable:

```{r}
icon <- mutate(icon,
               POS_simple = ifelse(POS %in% gram, 'Function', POS))
```

Check categories:

```{r}
table(icon$POS_simple)
```

Get a reduced POS data frame without names and unclassifieds. This will be used later to making computing averages easier.

```{r}
icon_POS <- filter(icon,
                   !POS_simple %in% c('Unclassified', 'Name'))
```

Check:

```{r}
table(icon_POS$POS_simple)
```


## Check overlap with different datasets for reporting

How many data points?

```{r}
filter(icon, !is.na(SER)) %>% nrow()
filter(icon, !is.na(AOA)) %>% nrow()
filter(icon, !is.na(humor)) %>% nrow()
filter(icon, !is.na(Aud_z)) %>% nrow()
```


## Descriptive statistics

Average iconicity for these:

```{r}
icon_POS %>% group_by(POS_simple) %>% 
  summarize(M = mean(rating),
            SD = sd(rating)) %>% 
  arrange(desc(M))
```

Check the dominant perceptual modality:

```{r}
icon %>% group_by(Mod) %>% 
  summarize(M = mean(rating),
            SD = sd(rating)) %>% 
  arrange(desc(M))
```

Check the dominant perceptual modality only for very perceptual words. First, get a subset:

```{r}
sense <- icon %>% filter(!is.na(Max_perceptual)) %>% 
  filter(Max_perceptual > quantile(Max_perceptual, 0.8))

# How many?

nrow(sense)
```

Then re-do the averages:

```{r}
sense %>% 
  group_by(Mod) %>% 
  summarize(M = mean(rating),
            SD = sd(rating)) %>% 
  arrange(desc(M))
```

Check Levin verb classes:

```{r}
levin <- left_join(levin, icon, by = c('verb' = 'word'))

# Get counts:

levin_counts <- levin %>% count(category)

# Averages:

levin_avg <- levin %>% group_by(category) %>% 
  summarize(M = mean(rating, na.rm = TRUE),
            SD = sd(rating, na.rm = TRUE)) %>% 
  arrange(desc(M))

# Put counts in there:

levin_avg <- left_join(levin_avg, levin_counts)

# Show:

levin_avg
```

For comparison, the least iconic classes:

```{r}
arrange(levin_avg, M)
```

Print the table:

```{r}
levin_avg <- mutate(levin_avg, M = round(M, 2),
                    SD = round(SD, 2))
write_csv(levin_avg, '../tables/levin_averages.csv')
```


## Correlation table

Get the variables of interest:

```{r}
these_vars <- c("rating", "SER", "AOA", "LogFreq", "LogCD",
                "logletterfreq", "humor", "NMorph")
```

Get these vars and perform pairwise correlations:

```{r}
# Get subset:

df_vars <- icon[, these_vars]

# Correlate and round:

all_corrs <- round(cor(df_vars, use = 'complete.obs'), 2)

# Print and save:

all_corrs
write_csv(as.data.frame(all_corrs), '../tables/all_correlations.csv')
```

Perform them again with subset of monomorphemics:

```{r}
# Get subset:

mono <- icon %>% filter(NMorph == 1)
mono <- mono[, these_vars]
mono <- mono[, -ncol(mono)]

# Correlate and round:

mono_corrs <- round(cor(df_vars, use = 'complete.obs'), 2)

# Print:

mono_corrs
```


## Interpret models

Load all models from the models folder:

```{r}
all_models <- list.files('../models/')

for (i in seq_along(all_models)) {
  load(str_c('../models/', all_models[i]))
}
```

First, let's look at all r-squareds:

```{r}
# Sensory experience and modality:

bayes_R2(lanc_mdl)
bayes_R2(lanc_80_mdl)
bayes_R2(lanc_max_mdl)

# Lupyan & Winter (2018) models:

bayes_R2(lupyan_pex_mdl)

# FUll model:

bayes_R2(all_mdl)
bayes_R2(all_mdl_no_POS)
bayes_R2(noweight_mdl)
bayes_R2(all_conc_mdl)

# Generality model:

bayes_R2(gen_mdl)
```

Next, let's look at the model summaries:

```{r}
# Morphology control variables:

summary(morph_mdl)
summary(logmorph_mdl)

# Sensory experience and modality:

summary(SER_mdl)
summary(lanc_mdl)
summary(lanc_80_mdl)
summary(lanc_max_mdl)

# Lupyan & Winter (2018) models:

summary(lupyan_CD_mdl)
summary(lupyan_semD_mdl)
summary(lupyan_semD_conc_mdl)
summary(lupyan_pex_mdl)

# FUll model:

summary(all_mdl)
summary(all_mdl_no_POS)

# Generality model

summary(gen_mdl)
```

Then again, full model with concreteness instead:

```{r}
summary(all_conc_mdl)
```

## Make a coefficient plot of the main model

Get the fixed effects:

```{r}
these_rows <- row.names(fixef(all_mdl))
fixefs <- as_tibble(fixef(all_mdl))
fixefs$variable <- these_rows

# Get rid of intercept:

fixefs <- fixefs[-1, ]

# Get rid of the POS predictors for the plot:

POS <- c('POS_simpleAdjective', 'POS_simpleAdverb',
         'POS_simpleInterjection', 'POS_simpleNoun',
         'POS_simpleVerb')
fixefs <- filter(fixefs,
                 !(variable %in% POS))

# Rename the variables:

fixefs <- mutate(fixefs,
                 variable = str_remove(variable, '_z'),
                 variable = ifelse(variable == 'humor',
                                   'humor ratings', variable),
                 variable = ifelse(variable == 'LogFreq',
                                   'log frequency', variable),
                 variable = ifelse(variable == 'LogCD',
                                   'log contextual diversity', variable),
                 variable = ifelse(variable == 'SER',
                                   'sensory experience ratings (SER)', variable),
                 variable = ifelse(variable == 'AOA',
                                   'age-of-acquisition ratings', variable),
                 variable = ifelse(variable == 'ARC',
                                   'ARC (semantic neighborhood density)', variable),
                 variable = ifelse(variable == 'SER:ARC_z',
                                   'ARC * SER interaction', variable),
                 variable = ifelse(variable == 'NMorph',
                                   'number of morphemes', variable),
                 variable = ifelse(variable == 'logletter',
                                   'log letter frequency', variable))

# Show tibble:

fixefs
```

Make a coefficient plot of this:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

coef_p <- fixefs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

coef_p <- coef_p + 
  geom_point(shape = 15) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

coef_p <- coef_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

coef_p

# Save:

ggsave(plot = coef_p, filename = '../figures/main_coefficients.pdf',
       width = 9, height = 5.5)
```

Create the predictions for POS categories:

```{r}
# Extract posteriors

POS_posts <- posterior_samples(POS_mdl)

# Add posterior samples of the respective coefficients

functions <- POS_posts$b_Intercept
adjs <- POS_posts$b_Intercept + POS_posts$b_POS_simpleAdjective
advbs <- POS_posts$b_Intercept + POS_posts$b_POS_simpleAdverb
interj <- POS_posts$b_Intercept + POS_posts$b_POS_simpleInterjection
nouns <- POS_posts$b_Intercept + POS_posts$b_POS_simpleNoun
verbs <- POS_posts$b_Intercept + POS_posts$b_POS_simpleVerb

# Create tibble:

preds <- tibble(POS = c('Function',
                        'Adverb',
                        'Noun',
                        'Adjective',
                        'Verb',
                        'Interjection'),
                Estimate = c(mean(functions),
                             mean(advbs),
                             mean(nouns),
                             mean(adjs),
                             mean(verbs),
                             mean(interj)),
                Q2.5 = c(quantile(functions, 0.025),
                         quantile(advbs, 0.025),
                         quantile(nouns, 0.025),
                         quantile(adjs, 0.025),
                         quantile(verbs, 0.025),
                         quantile(interj, 0.025)),
                Q97.5 = c(quantile(functions, 0.975),
                          quantile(advbs, 0.975),
                          quantile(nouns, 0.975),
                          quantile(adjs, 0.975),
                          quantile(verbs, 0.975),
                          quantile(interj, 0.975)))

# Show:

preds
```

Make a prediction plot of this:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

POS_p <- preds %>%
  ggplot(aes(x = reorder(POS, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

POS_p <- POS_p + 
  geom_point(shape = 15, size = 2) +
  geom_errorbar(width = 0.1)

# Add cosmetics:

POS_p <- POS_p +
  ylab('Estimated iconicity') +
  theme_minimal() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(face = 'bold',
                                   size = 12, angle = 45, hjust = 1),
        axis.title.y = element_text(margin = margin(t = 0, b = 0,
                                                    r = 15, l = 0),
                                    face = 'bold', size = 16))

# Show plot:

POS_p

# Save:

ggsave(plot = POS_p, filename = '../figures/POS_preds.pdf',
       width = 7, height = 7)
```

Make a coefficient plot of the sensory modality effects. First, get the coefficients of the 80th percentile model:

```{r}
# Get coefficients:

lancs <- as_tibble(fixef(lanc_80_mdl)[-1, ]) # minus intercept

# Rename variables:

lancs$variable <- c('auditory strength',
                    'gustatory strength',
                    'haptic strength',
                    'interoceptive strength',
                    'olfactory strength',
                    'visual strength',
                    # 'max perceptual strength',
                    'modality exclusivity')

# Show:

lancs
```

Then make the plot:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

lancs_p <- lancs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

lancs_p <- lancs_p + 
  geom_point(shape = 15, size = 2) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

lancs_p <- lancs_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

lancs_p

# Save:

ggsave(plot = lancs_p, filename = '../figures/lancs_80_coefficients.pdf',
       width = 8, height = 6)
```

Next get the coefficients of the general model (no exclusions):

```{r}
# Get coefficients:

lancs <- as_tibble(fixef(lanc_mdl)[-1, ]) # minus intercept

# Rename variables:

lancs$variable <- c('auditory strength',
                    'gustatory strength',
                    'haptic strength',
                    'interoceptive strength',
                    'olfactory strength',
                    'visual strength',
                    'max perceptual strength',
                    'modality exclusivity')

# Show:

lancs
```

Then make the plot:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

lancs_p <- lancs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

lancs_p <- lancs_p + 
  geom_point(shape = 15, size = 2) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

lancs_p <- lancs_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

lancs_p

# Save:

ggsave(plot = lancs_p, filename = '../figures/lancs_coefficients.pdf',
       width = 8, height = 6)
```

## Look at Beth Levin verb classes (new results)

First, check Bayes R2:

```{r}
bayes_R2(levin_mdl)
```

Check the loo compare for the model:

```{r}
levin_loo_compare
```
