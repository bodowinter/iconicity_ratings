---
title: "Iconicity ratings - substantive analysis"
author: "Bodo"
date: "11/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This markdown performs a number of different analysis that serve to demonstrate how the iconicity ratings correlate with other psycholinguistic norms that have been collected. In addition, we produce some plots of the data to give an overview of the ratings.

The models are computed in the file "Bayesian_models.Rmd" and will be loaded in and interpreted here.

## Data and package loading

Load packages:

```{r, warning = FALSE, message = FALSE}
library(brms)
library(tidyverse)
library(patchwork)
library(ggwordcloud)
```

For reproducibility:

```{r}
packageVersion('brms')
packageVersion('tidyverse')
packageVersion('patchwork')
packageVersion('ggwordcloud')
R.Version()$version.string
```

Load iconicity ratings:

```{r, warning = FALSE, message = FALSE}
icon <- read_csv('../ratings/iconicity_ratings.csv')
```

Load additional datasets:

```{r, warning = FALSE, message = FALSE}
# Data for replications:

SER <- read_csv('../additional_data/juhasz_yap_2013_SER.csv')
AOA <- read_csv('../additional_data/kuperman_2012_AOA.csv')
lanc <- read_csv('../additional_data/lancaster_sensorimotor_norms_2019.csv')
SUBTL <- read_csv('../additional_data/brysbaert_2012_SUBTLEX_POS.csv')
humor <- read_csv('../additional_data/engelthaler_hills_2018_humor.csv')
ding <- read_csv('../additional_data/dingemanse_thompson_2020.csv')

# Data for controlling morphology:

ELP <- read_csv('../additional_data/balota_2007_ELP.csv')

# Data for new analyses:

amsel <- read_csv('../additional_data/amsel_2012_SER.csv')
wisc <- read_csv('../additional_data/wisconsin_2005_norms.csv')
levin <- read_csv('../additional_data/levin_1991_verb_classes.csv')
```

Rename folders and simplify data frames to include only relevant info. Also make Lancaster norm Word column lowercase. Log10 transform SUBTLEX frequencies and contextual diversity:

```{r}
# Age-of-acquisition data:

AOA <- AOA %>% select(Word, Rating.Mean) %>% 
  rename(AOA = Rating.Mean)

# Sensory experience ratings:

SER <- select(SER, Word, SER)

# Lancaster sensorimotor norms:

lanc <- lanc %>%
  mutate(Word = str_to_lower(Word)) %>% 
  select(Word:Visual.mean, Dominant.perceptual,
         Max_strength.perceptual) %>% 
  rename(Aud = Auditory.mean,
         Gus = Gustatory.mean,
         Hap = Haptic.mean,
         Int = Interoceptive.mean,
         Olf = Olfactory.mean,
         Vis = Visual.mean,
         Mod = Dominant.perceptual,
         Max_perceptual = Max_strength.perceptual)

# Frequency, contextual diversity, and part-of-speech:

SUBTL <- SUBTL %>% 
  rename(Freq = FREQcount,
         CD = CDcount,
         POS = Dom_PoS_SUBTLEX) %>% 
  select(Word, Freq, CD, POS)

# Playfulness:
  
humor <- select(humor, word, mean) %>%
  rename(humor = mean)

# Dingemanse & Thompson (2020) data:

ding <- select(ding, logletterfreq, word, ico, ico_imputed, ico_imputed_monomorph)

# Amsel (2012) and Wisconsin (2005) data:

amsel <- select(amsel, Concept, Smell, Color, Taste, Sound, Motion) %>% 
  rename(amsel_smell = Smell, amsel_color = Color,
         amsel_taste = Taste, amsel_sound = Sound,
         amsel_motion = Motion)
wisc <- select(wisc, Word, SoundMean, ColorMean, MotionMean) %>%
  rename(wisc_sound = SoundMean, wisc_color = ColorMean, wisc_motion = MotionMean)


# ELP data:

ELP <- select(ELP, Word, NMorph) %>% 
  mutate(LogMorph = log10(NMorph),
         Word = str_to_lower(Word))
```

Join them into the main iconicity data file:

```{r}
icon <- left_join(icon, SER, by = c('word' = 'Word'))
icon <- left_join(icon, AOA, by = c('word' = 'Word'))
icon <- left_join(icon, SUBTL, by = c('word' = 'Word'))
icon <- left_join(icon, humor, by = c('word' = 'word'))
icon <- left_join(icon, lanc, by = c('word' = 'Word'))
icon <- left_join(icon, ding, by = c('word' = 'word'))
icon <- left_join(icon, wisc, by = c('word' = 'Word'))
icon <- left_join(icon, amsel, by = c('word' = 'Concept'))
icon <- left_join(icon, ELP, by = c('word' = 'Word'))
```

For SUBTLEX, an NA is a true zero:

```{r}
icon <- mutate(icon,
               Freq = ifelse(is.na(Freq), 0, Freq),
               CD = ifelse(is.na(CD), 0, CD))
```

Log-transform the frequencies:

```{r}
icon <- mutate(icon,
               LogFreq = log10(Freq + 1),
               LogCD = log10(CD + 1))
```

Z-score all variables:

```{r}
z_score <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x)

icon <- mutate(icon,
               SER_z = z_score(SER),
               AOA_z = z_score(AOA),
               LogFreq_z = z_score(LogFreq),
               LogCD_z = z_score(LogCD),
               humor_z = z_score(humor),
               logletter_z = z_score(logletterfreq),
               lognmorph_z = z_score(LogMorph),
               
               amsel_color_z = z_score(amsel_color),
               amsel_smell_z = z_score(amsel_smell),
               amsel_taste_z = z_score(amsel_taste),
               amsel_sound_z = z_score(amsel_sound),
               amsel_motion_z = z_score(amsel_motion),
               
               wisc_sound_z = z_score(wisc_sound),
               wisc_color_z = z_score(wisc_color),
               wisc_motion_z = z_score(wisc_motion),
               
               # Lancaster norms:
               Aud_z = z_score(Aud),
               Gus_z = z_score(Gus),
               Hap_z = z_score(Hap),
               Int_z = z_score(Int),
               Olf_z = z_score(Olf),
               Vis_z = z_score(Vis))
```

## Process part of speech tags

Process the part-of-speech information to collapse categories for better representation. First show what categories there are:

```{r}
sort(table(icon$POS))
```

Define vector of stuff to set as function words. "Ex" = there. "#N/A" are words like "gonna", "wanna". 

```{r}
gram <- c('#N/A', 'Article', 'Conjunction',
          'Determiner', 'Not', 'Number',
          'Preposition', 'Pronoun', 'To',
          'Ex')
```

Set this to function words in a new POS variable:

```{r}
icon <- mutate(icon,
               POS_simple = ifelse(POS %in% gram, 'Function', POS))
```

Check categories:

```{r}
table(icon$POS_simple)
```

Get a reduced POS data frame without names and unclassifieds. This will be used later to making computing averages easier.

```{r}
icon_POS <- filter(icon,
                   !POS_simple %in% c('Unclassified', 'Name'))
```

Check:

```{r}
table(icon_POS$POS_simple)
```

## Create a word cloud

Create a word cloud. First, extract the first 100 most iconic words:

```{r}
cloud_df <- icon %>% arrange(desc(rating)) %>% 
  slice_head(n = 100)
```


```{r, fig.width = 14, fig.height = 15, warning = FALSE, message = FALSE}
set.seed(666)

cloud_p <- cloud_df %>% 
  ggplot(aes(label = word,
             # color = rating,
             size = rating)) +
  geom_text_wordcloud_area(eccentricity = 0.9) +
  scale_size_area(max_size = 10) +
  # scale_color_gradient(low = '#5382b0', high = '#c79b3c' ) +
  theme_minimal()

# Show:

cloud_p
ggsave(plot = cloud_p, filename = '../figures/wordcloud.pdf',
       width = 11, height = 7)
```

## Correlate with  old ratings and imputed ratings

Check against imputed ratings from the Dingemanse & Thompson (2020) file:

```{r}
with(icon, cor(rating, ico_imputed, use = 'complete.obs'))
with(icon, cor(rating, ico_imputed_monomorph, use = 'complete.obs'))
```

Check against old ratings from Dingemanse & Thompson (2020) file - careful, this is a subset:

```{r}
with(icon, cor(rating, ico, use = 'complete.obs'))
```



## Check the distribution for reporting

For reporting, 10 most iconic words:

```{r}
arrange(icon, desc(rating))
```

And 10 least iconic words:

```{r}
arrange(icon, rating)
```

First, mean and SD:

```{r}
icon %>% summarize(M = mean(rating),
                   SD = sd(rating),
                   SD = round(SD, 2))
```

First, the iconicity distribution plot:

```{r, fig.width = 8, fig.height = 6}
# Main plot with mappings:

icon_p <- icon %>%
  ggplot(aes(x = rating))

# Add normal curve:

icon_p <- icon_p + 
  stat_function(fun = dnorm,
                args = list(mean = mean(icon$rating),
                            sd = sd(icon$rating)),
                col = 'black',
                linetype = 2)

# Add density geom:

icon_p <- icon_p + 
  geom_density(fill = 'steelblue', alpha = 0.7, col = 'black')

# Add cosmetics:

icon_p <- icon_p + 
  ylim(0, 0.5) + 
  xlab('Iconicity rating') +
  ylab('Density') +
  theme_minimal() +
  
  # Axis labels:
  theme(axis.title.x = element_text(face = 'bold',
                                    size = 16,
                                    margin = margin(t = 10, b = 0,
                                                    l = 0, r = 0)),
        axis.title.y = element_text(face = 'bold',
                                    size = 16,
                                    margin = margin(r = 15, l = 0,
                                                    t = 0, b = 0))) +
  
  # Axis tick marks:
  theme(axis.text.x = element_text(face = 'bold',
                                   size = 12),
        axis.text.y = element_text(face = 'bold',
                                   size = 12))

# Show in markdown:

icon_p

# Save:

ggsave(plot = icon_p,
       filename = '../figures/iconicity_ratings_density.pdf',
       width = 8, height = 6)
```

Create a Q-Q plot. First, get the quartiles of this distribution (x), against the quartiles of the normal (y):

```{r}
QQ <- tibble(x = qqnorm(icon$rating, plot = FALSE)$x,
             y = qqnorm(icon$rating, plot = FALSE)$y)
```

Make a plot out of this:

```{r, fig.width = 8, fig.height = 6}
# Main plot:

qq_p <- QQ %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = 0.2)

# Add cosmetics:

qq_p <- qq_p + 
  xlab('Theoretical quantiles') +
  ylab('Sample quantiles') +
  theme_minimal() +
  
  # Axis labels:
  theme(axis.title.x = element_text(face = 'bold',
                                    size = 16,
                                    margin = margin(t = 10, b = 0,
                                                    l = 0, r = 0)),
        axis.title.y = element_text(face = 'bold',
                                    size = 16,
                                    margin = margin(r = 15, l = 0,
                                                    t = 0, b = 0))) +
  
  # Axis tick marks:
  theme(axis.text.x = element_text(face = 'bold',
                                   size = 12),
        axis.text.y = element_text(face = 'bold',
                                   size = 12))

# Show in markdown:

qq_p

# Save:

ggsave(plot = qq_p,
       filename = '../figures/QQ_plot.pdf',
       width = 8, height = 6)
```

Create the Pollock (2018) style plot:

```{r, fig.width = 8, fig.height = 6}
# Main plot:

pollock_p <- icon %>%
  ggplot(aes(x = rating, y = rating_sd)) +
  geom_point(alpha = 0.2)

# Add cosmetics:

pollock_p <- pollock_p + 
  xlab('Mean') +
  ylab('Standard deviation') +
  theme_minimal() +
  
  # Axis labels:
  theme(axis.title.x = element_text(face = 'bold',
                                    size = 16,
                                    margin = margin(t = 10, b = 0,
                                                    l = 0, r = 0)),
        axis.title.y = element_text(face = 'bold',
                                    size = 16,
                                    margin = margin(r = 15, l = 0,
                                                    t = 0, b = 0))) +
  
  # Axis tick marks:
  theme(axis.text.x = element_text(face = 'bold',
                                   size = 12),
        axis.text.y = element_text(face = 'bold',
                                   size = 12))

# Show in markdown:

pollock_p

# Save:

ggsave(plot = pollock_p,
       filename = '../figures/pollock_2018.pdf',
       width = 8, height = 6)
```

Create a double plot of normal distribution with Pollock (2018) graph:

```{r, fig.width = 12, fig.height = 6}
# Add titles:

icon_p <- icon_p +
  ggtitle('(a) Iconicity rating distribution') +
  theme(title = element_text(face = 'bold',
                             size = 18,
                             margin = margin(t = 0, b = 15,
                                             r = 0, l = 0)))

pollock_p <- pollock_p +
  ggtitle('(b) Standard deviation by mean') +
  theme(title = element_text(face = 'bold',
                             size = 18,
                             margin = margin(t = 0, b = 15,
                                             r = 0, l = 0)))

# Put into plot together:

figure_1 <- icon_p + plot_spacer() + pollock_p +
  plot_layout(widths = c(8, 1, 8))


# Show in script:

figure_1

# Save:

ggsave(plot = figure_1,
       filename = '../figures/figure1.pdf',
       width = 12, height = 5)
```

## Check overlap with different datasets for reporting

How many data points?

```{r}
filter(icon, !is.na(SER)) %>% nrow()
filter(icon, !is.na(AOA)) %>% nrow()
filter(icon, !is.na(humor)) %>% nrow()
filter(icon, !is.na(Aud_z)) %>% nrow()
```


## Descriptive statistics

Average iconicity for these:

```{r}
icon_POS %>% group_by(POS_simple) %>% 
  summarize(M = mean(rating),
            SD = sd(rating)) %>% 
  arrange(desc(M))
```

Check the dominant perceptual modality:

```{r}
icon %>% group_by(Mod) %>% 
  summarize(M = mean(rating),
            SD = sd(rating)) %>% 
  arrange(desc(M))
```

Check the dominant perceptual modality only for very perceptual words. First, get a subset:

```{r}
sense <- icon %>% filter(!is.na(Max_perceptual)) %>% 
  filter(Max_perceptual > quantile(Max_perceptual, 0.8))

# How many?

nrow(sense)
```

Then re-do the averages:

```{r}
sense %>% 
  group_by(Mod) %>% 
  summarize(M = mean(rating),
            SD = sd(rating)) %>% 
  arrange(desc(M))
```

Check Levin verb classes:

```{r}
levin <- left_join(levin, icon, by = c('verb' = 'word'))

# Get counts:

levin_counts <- levin %>% count(category)

# Averages:

levin_avg <- levin %>% group_by(category) %>% 
  summarize(M = mean(rating, na.rm = TRUE),
            SD = sd(rating, na.rm = TRUE)) %>% 
  arrange(desc(M))

# Put counts in there:

levin_avg <- left_join(levin_avg, levin_counts)

# Show:

levin_avg
```

For comparison, the least iconic classes:

```{r}
arrange(levin_avg, M)
```

## Correlation table

Get the variables of interest:

```{r}
these_vars <- c("rating", "SER", "AOA", "LogFreq", "LogCD", "logletterfreq", "humor", "NMorph")
```

Get these vars and perform pairwise correlations:

```{r}
# Get subset:

df_vars <- icon[, these_vars]

# Correlate and round:

all_corrs <- round(cor(df_vars, use = 'complete.obs'), 2)

# Print and save:

all_corrs
write_csv(as.data.frame(all_corrs), '../tables/all_correlations.csv')
```

Perform them again with subset of monomorphemics:

```{r}
# Get subset:

mono <- icon %>% filter(NMorph == 1)
mono <- mono[, these_vars]
mono <- mono[, -ncol(mono)]

# Correlate and round:

mono_corrs <- round(cor(df_vars, use = 'complete.obs'), 2)

# Print:

mono_corrs
```

Performing correlations on 


## Interpret models

Load all models from the models folder:

```{r}
all_models <- list.files('../models/')

for (i in seq_along(all_models)) {
  load(str_c('../models/', all_models[i]))
}
```

First, let's look at all r-squareds:

```{r}
# Morphology control variables:

bayes_R2(morph_mdl)
bayes_R2(logmorph_mdl)

# Sensory experience and modality:

bayes_R2(SER_mdl)
bayes_R2(conc_mdl)
bayes_R2(lanc_mdl)
bayes_R2(lanc_80_mdl)
bayes_R2(lanc_max_mdl)

# Lupyan & Winter (2018) models:

bayes_R2(lupyan_CD_mdl)
bayes_R2(lupyan_semD_mdl)
bayes_R2(lupyan_semD_conc_mdl)
bayes_R2(lupyan_pex_mdl)

# Age of acquisition:

bayes_R2(AOA_mdl)

# Dingemanse & Thompson (2020):

bayes_R2(humor_mdl)
bayes_R2(logletter_mdl)

# Frequency and contextual diversity:

bayes_R2(freq_mdl)
bayes_R2(freq_CD_mdl)

# Part-of-speech:

bayes_R2(POS_mdl)

# FUll model:

bayes_R2(all_mdl)
bayes_R2(all_mdl_no_POS)
```

Next, let's look at the model summaries:

```{r}
# Morphology control variables:

summary(morph_mdl)
summary(logmorph_mdl)

# Sensory experience and modality:

summary(SER_mdl)
summary(lanc_mdl)
summary(lanc_80_mdl)
summary(lanc_max_mdl)

# Lupyan & Winter (2018) models:

summary(lupyan_CD_mdl)
summary(lupyan_semD_mdl)
summary(lupyan_semD_conc_mdl)
summary(lupyan_pex_mdl)

# Age of acquisition:

summary(AOA_mdl)

# Dingemanse & Thompson (2020):

summary(humor_mdl)
summary(logletter_mdl)

# Frequency and contextual diversity:

summary(freq_mdl)
summary(freq_CD_mdl)

# Part-of-speech:

summary(POS_mdl)

# FUll model:

summary(all_mdl)
summary(all_mdl_no_POS)
```

Then again, full model with concreteness instead:

```{r}
summary(all_conc_mdl)
summary(all_conc_mdl_no_POS)
```


## Inference for categorical variables

Part of speech (WAIC here):

```{r}
WAIC(POS_mdl)
WAIC(POS_null)
```

Part of speech from main model:

```{r}
WAIC(all_mdl)
WAIC(all_mdl_no_POS)
```

Levin (WAIC here):

```{r}
WAIC(levin_mdl)
WAIC(levin_null_mdl)
```



## Make a coefficient plot of the main model

Get the fixed effects:

```{r}
these_rows <- row.names(fixef(all_mdl_no_POS))
fixefs <- as_tibble(fixef(all_mdl_no_POS))
fixefs$variable <- these_rows

# Get rid of intercept:

fixefs <- fixefs[-1, ]

# Rename the variables:

fixefs <- mutate(fixefs,
                 variable = str_remove(variable, '_z'),
                 variable = ifelse(variable == 'humor', 'humor ratings', variable),
                 variable = ifelse(variable == 'LogFreq', 'log frequency', variable),
                 variable = ifelse(variable == 'LogCD', 'log contextual diversity', variable),
                 variable = ifelse(variable == 'SER', 'sensory experience ratings', variable),
                 variable = ifelse(variable == 'AOA', 'age-of-acquisition ratings', variable),
                 variable = ifelse(variable == 'ARC', 'ARC (semantic neighborhood density)', variable),
                 variable = ifelse(variable == 'NMorph', 'number of morphemes', variable),
                 variable = ifelse(variable == 'logletter', 'log letter frequency', variable))

# Show tibble:

fixefs
```

Make a subset without freq / CD:

```{r}
# fixefs <- fixefs %>%
#   filter(!variable %in% c('log frequency', 'log contextual diversity'))
```


Make a coefficient plot of this:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

coef_p <- fixefs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

coef_p <- coef_p + 
  geom_point(shape = 15) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

coef_p <- coef_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

coef_p

# Save:

ggsave(plot = coef_p, filename = '../figures/main_coefficients.pdf',
       width = 9, height = 6)
```

Create the predictions for POS categories:

```{r}
# Extract posteriors

POS_posts <- posterior_samples(POS_mdl)

# Add posterior samples of the respective coefficients

functions <- POS_posts$b_Intercept
adjs <- POS_posts$b_Intercept + POS_posts$b_POS_simpleAdjective
advbs <- POS_posts$b_Intercept + POS_posts$b_POS_simpleAdverb
interj <- POS_posts$b_Intercept + POS_posts$b_POS_simpleInterjection
nouns <- POS_posts$b_Intercept + POS_posts$b_POS_simpleNoun
verbs <- POS_posts$b_Intercept + POS_posts$b_POS_simpleVerb

# Create tibble:

preds <- tibble(POS = c('Function',
                        'Adverb',
                        'Noun',
                        'Adjective',
                        'Verb',
                        'Interjection'),
                Estimate = c(mean(functions),
                             mean(advbs),
                             mean(nouns),
                             mean(adjs),
                             mean(verbs),
                             mean(interj)),
                Q2.5 = c(quantile(functions, 0.025),
                         quantile(advbs, 0.025),
                         quantile(nouns, 0.025),
                         quantile(adjs, 0.025),
                         quantile(verbs, 0.025),
                         quantile(interj, 0.025)),
                Q97.5 = c(quantile(functions, 0.975),
                          quantile(advbs, 0.975),
                          quantile(nouns, 0.975),
                          quantile(adjs, 0.975),
                          quantile(verbs, 0.975),
                          quantile(interj, 0.975)))

# Show:

preds
```

Make a prediction plot of this:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

POS_p <- preds %>%
  ggplot(aes(x = reorder(POS, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

POS_p <- POS_p + 
  geom_point(shape = 15, size = 2) +
  geom_errorbar(width = 0.1)

# Add cosmetics:

POS_p <- POS_p +
  ylab('Estimated iconicity') +
  theme_minimal() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(face = 'bold',
                                   size = 12, angle = 45, hjust = 1),
        axis.title.y = element_text(margin = margin(t = 0, b = 0,
                                                    r = 15, l = 0),
                                    face = 'bold', size = 16))

# Show plot:

POS_p

# Save:

ggsave(plot = POS_p, filename = '../figures/POS_preds.pdf',
       width = 7, height = 7)
```

Make a coefficient plot of the sensory modality effects. First, get the coefficients:

```{r}
# Get coefficients:

lancs <- as_tibble(fixef(lanc_mdl)[-1, ]) # minus intercept

# Rename variables:

lancs$variable <- c('auditory strength',
                    'gustatory strength',
                    'haptic strength',
                    'interoceptive strength',
                    'olfactory strength',
                    'visual strength')

# Show:

lancs
```

Then make the plot:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

lancs_p <- lancs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

lancs_p <- lancs_p + 
  geom_point(shape = 15, size = 2) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

lancs_p <- lancs_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

lancs_p

# Save:

ggsave(plot = lancs_p, filename = '../figures/lancs_coefficients.pdf',
       width = 8, height = 6)
```

## Look at the new variables

First, check Bayes R2:

```{r}
bayes_R2(amsel_mdl)
bayes_R2(wisc_mdl)
bayes_R2(levin_mdl)
```

Next, make a plot with Amsel (2012) results. Get the coefficients:

```{r}
amsel_coefs <- as_tibble(fixef(amsel_mdl)[-1, ])
amsel_coefs$variable <- c('color ratings', 'taste ratings',
                          'sound ratings', 'smell ratings',
                          'motion ratings')

# Show:

amsel_coefs
```

Make the plot:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

amsel_p <- amsel_coefs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

amsel_p <- amsel_p + 
  geom_point(shape = 15, size = 2) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

amsel_p <- amsel_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

amsel_p

# Save:

ggsave(plot = amsel_p, filename = '../figures/amsel_2012_coefficients.pdf',
       width = 8, height = 6)
```


Next, make a plot with Wisconsin (2005) results. Get the coefficients:

```{r}
wisc_coefs <- as_tibble(fixef(wisc_mdl)[-1, ])
wisc_coefs$variable <- c('sound ratings', 'color ratings',
                          'motion ratings')

# Show:

wisc_coefs
```

Make the plot:

```{r, fig.width = 5, fig.height = 8}
# Setup the plot:

wisc_p <- wisc_coefs %>%
  ggplot(aes(x = reorder(variable, Estimate), y = Estimate,
             ymin = Q2.5, ymax = Q97.5))

# Add geoms:

wisc_p <- wisc_p + 
  geom_point(shape = 15, size = 2) +
  geom_hline(aes(yintercept = 0), linetype = 2) + 
  geom_errorbar(width = 0.1)

# Add cosmetics:

wisc_p <- wisc_p +
  ylab('Standardized coefficient') +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_text(margin = margin(t = 15, b = 0,
                                                    r = 0, l = 0),
                                    face = 'bold',
                                    size = 16),
        axis.text.y = element_text(face = 'bold', size = 14))

# Show plot:

wisc_p

# Save:

ggsave(plot = wisc_p, filename = '../figures/wisconsin_2005_coefficients.pdf',
       width = 8, height = 6)
```

Make a double plot of this:

```{r, fig.width = 12, fig.height = 6}
# Add titles:

amsel_p <- amsel_p +
  ggtitle('(a) Amsel et al. (2012) norms') +
  theme(title = element_text(face = 'bold',
                             size = 18,
                             margin = margin(t = 0, b = 15,
                                             r = 0, l = 0)))

wisc_p <- wisc_p +
  ggtitle('(b) Wisconsin norms') +
  theme(title = element_text(face = 'bold',
                             size = 18,
                             margin = margin(t = 0, b = 15,
                                             r = 0, l = 0)))

# Put into plot together:

amsel_wisc_p <- amsel_p + plot_spacer() + wisc_p +
  plot_layout(widths = c(8, 1, 8))


# Show in script:

amsel_wisc_p

# Save:

ggsave(plot = amsel_wisc_p,
       filename = '../figures/amsel_wisconsin_double_plot.pdf',
       width = 12, height = 5)
```



